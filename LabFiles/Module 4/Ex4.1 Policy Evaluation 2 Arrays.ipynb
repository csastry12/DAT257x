{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# DAT257x: Reinforcement Learning Explained\n\n## Lab 4: Dynamic Programming\n\n### Exercise 4.1 Policy Evaluation with 2 Arrays"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Policy Evaluation calculates the value function for a policy, given the policy and the full definition of the associated Markov Decision Process.  The full definition of an MDP is the set of states, the set of available actions for each state, the set of rewards, the discount factor, and the state/reward transition function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import test_dp               # required for testing and grading your code\nimport gridworld_mdp as gw   # defines the MDP for a 4x4 gridworld",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The gridworld MDP defines the probability of state transitions for our 4x4 gridworld using a \"get_transitions()\" function.  \n\nLet's try it out now, with state=2 and all defined actions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# try out the gw.get_transitions(state, action) function\n\nstate = 2\nactions = gw.get_available_actions(state)\n\nfor action in actions:\n    transitions = gw.get_transitions(state=state, action=action)\n\n    # examine each return transition (only 1 per call for this MDP)\n    for (trans) in transitions:\n        next_state, reward, probability = trans    # unpack tuple\n        print(\"transition(\"+ str(state) + \", \" + action + \"):\", \"next_state=\", next_state, \", reward=\", reward, \", probability=\", probability)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "transition(2, up): next_state= 2 , reward= -1 , probability= 1\ntransition(2, down): next_state= 6 , reward= -1 , probability= 1\ntransition(2, left): next_state= 1 , reward= -1 , probability= 1\ntransition(2, right): next_state= 3 , reward= -1 , probability= 1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Implement the algorithm for Iterative Policy Evaluation using the 2 array approach**.  In the 2 array approach, one array holds the value estimates for each state computed on the previous iteration, and one array holds the value estimates for the states computing in the current iteration.\n\nA empty function **policy_eval_two_arrays** is provided below; implement the body of the function to correctly calculate the value of the policy using the 2 array approach.  The function defines 5 parameters - a definition of each parameter is given in the comment block for the function.  For sample parameter values, see the calling code in the cell following the function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import inspect\nprint(inspect.signature(gw.get_available_actions))\nprint(inspect.signature(gw.get_transitions))",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(state)\n(state, action)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x1 = [1, 2, 3, 4, 5]\nx2 = [6, 7, 8, 9, 10]\n\nprint(x2)\nx1[3] = 25\nx1[4] = 35\nx2 = x1\nprint(x2)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[6, 7, 8, 9, 10]\n[1, 2, 3, 25, 35]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom numpy import linalg as LA\n\ndef policy_eval_two_arrays(state_count, gamma, theta, get_policy, get_transitions):\n    \"\"\"\n    This function uses the two-array approach to evaluate the specified policy for the specified MDP:\n    \n    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n    \n    'gamma' is the MDP discount factor for rewards.\n    \n    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n    \n    'get_policy' is the stochastic policy function - it takes a state parameter and returns list of tuples, \n        where each tuple is of the form: (action, probability).  It represents the policy being evaluated.\n        \n    'get_transitions' is the state/reward transiton function.  It accepts two parameters, state and action, and returns\n        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n        \n    \"\"\"\n    \n    Vk = state_count*[0]\n    Vkplus1 = state_count*[0]\n    \n    #\n    # INSERT CODE HERE to evaluate the policy using the 2 array approach \n    #\n    \n    while True:\n        \n        delta = 0\n        \n        for i in range(state_count):\n        \n            # Get policy for each state and unpack\n\n            action_probs = get_policy(i)\n            \n            # Loop over each tuple in action_probs array\n\n            out_sum = 0\n            \n            \n            for (action_prob) in action_probs:\n                \n                # Unpack each action_prob tuple and get action and its probability\n                \n                action, prob = action_prob\n\n                # Use action, to get array of transitions\n\n                transitions = gw.get_transitions(state=i, action=action)\n\n                # loop over eacch possible transition to get next state, reward, and probability of transition\n\n                inner_sum = 0\n                \n                for (trans) in transitions:\n\n                    next_state, reward, probability = trans    # unpack tuple\n                    \n                    # Calculate sigma(r, sprime) * p(r, sprime | s, a)\n                    \n                    inner_sum = inner_sum + probability * (reward + gamma * Vk[next_state])\n                    \n                # calculate sigma(pi(a | s) * sigma(r, sprime) * p(r, sprime | s, a))\n                    \n                out_sum = out_sum + prob * inner_sum\n                \n            Vkplus1[i] = out_sum\n            \n            delta = max(delta, abs(Vk[i] - Vkplus1[i]))\n            \n        # check exit condition\n        \n #       if LA.norm((np.array(Vk) - np.array(Vkplus1)), 1) < theta:\n        if delta < theta:\n            \n            # Be careful. This is dangerous because Vkplus1 will and Vk no reference the same list whose elements will be modified \n            # next iteration. Instead make a copy of Vkplus1 and then assign to Vk so it remains unchanged until this assignment\n#            Vk = Vkplus1 \n            Vk = Vkplus1.copy()\n            break\n            \n        else:\n            \n            Vk = Vkplus1.copy()\n                \n    return Vk",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, test our function using the MDP defined by gw.* functions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_equal_policy(state):\n    # build a simple policy where all 4 actions have the same probability, ignoring the specified state\n    policy = ( (\"up\", .25), (\"right\", .25), (\"down\", .25), (\"left\", .25))\n    return policy\n\nn_states = gw.get_state_count()\n\n# test our function\nvalues = policy_eval_two_arrays(state_count=n_states, gamma=.9, theta=.001, get_policy=get_equal_policy, \\\n    get_transitions=gw.get_transitions)\n\nprint(\"Values=\", values)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Values= [0.0, -5.274709263277986, -7.123800104889248, -7.64536148969558, -5.274709263277987, -6.602238720082915, -7.17604178238719, -7.1238001048892485, -7.1238001048892485, -7.176041782387191, -6.602238720082915, -5.274709263277986, -7.645361489695581, -7.1238001048892485, -5.274709263277986]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Expected output from running above cell:**\n\n`\nValues= [0.0, -5.274709263277986, -7.123800104889248, -7.64536148969558, -5.274709263277987, -6.602238720082915, -7.17604178238719, -7.1238001048892485, -7.1238001048892485, -7.176041782387191, -6.602238720082915, -5.274709263277986, -7.645361489695581, -7.1238001048892485, -5.274709263277986]\n`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\na = np.append(values, 0)\nnp.reshape(a, (4,4))",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "array([[ 0.        , -1.675     , -2.051875  , -2.13667187],\n       [-1.675     , -2.20375   , -2.40751563, -2.47244219],\n       [-2.051875  , -2.40751562, -2.53338203, -2.35131045],\n       [-2.13667188, -2.47244219, -2.35131045,  0.        ]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, test our function using the test_dp helper.  The helper also uses the gw MDP, but with a different gamma value.\nIf our function passes all tests, a passcode will be printed."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# test our function using the test_db helper\ntest_dp.policy_eval_two_arrays_test( policy_eval_two_arrays ) ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}